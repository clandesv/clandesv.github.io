[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Please enjoy exploring my portfolio to learn more about my person and to see a selection of my previous work and projects."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "üëã Greetings! My name is Camille Landesvatter.\nüë©üèΩ‚Äçüíª For the last 3.5 years I have been working as a Quantitative Researcher at the Mannheim Centre for European Social Research (MZES) at the University of Mannheim.\nüè´ I studied Social Sciences and Sociology at the University of Stuttgart and Mannheim. I chose my courses, coursework and thesis within the intersection of survey research, statistics and data science.\nüéì In April 2024 I earned my PhD in Quantitative Sociology at the Department for Social Data Science and Methodology (UMA).\n\nThesis Title: ‚ÄúMethods for the Classification of Data from Open-Ended Questions in Surveys‚Äù\nGPA: 1.5 (Magna Cum Laude)\nüå∞ My PhD in a Nutshell (Disputation Talk): Methods for the Classification of Data from Open-Ended Questions in Surveys\n\nüìç I currently live in Berlin, Germany and soon want to explore another European country.\nüîé I am currently exploring opportunities in Data Analysis and Data Science for my next professional chapter."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "For the latest overview on which projects I am working on recently, please also refer to my GitHub Account.\n\nAPIs for Social Scientists: A collaborative review\nTogether with students and researchers from the University of Mannheim and beyond, we created a comprehensive introduction that explores how social scientists can leverage APIs to address significant research questions. This introduction covers various use cases such as data collection and data manipulation but also a chapter on Authentication and other Best Practices for API usage. Together with my Co-Editors, I was responsible for the project management and coordination of over 20 researchers working together on this project.\nThe project is ongoing, and you can find the latest version here.\nTo create further resources out of our work, I collaborated with the MZES Social Science Data Lab to organize a roundtable event where together with co-editors and co-authors we reported on our experiences. You can watch the event on YouTube.\n\n\nA 2-day Workshop on Computational Social Science\ntba.\n\n\n\nHow do Social Scientists Visualize?\ntba.\nTogether with a colleague from the University of Freiburg and LMU Munich, and our student research assistant at the Mannheim Centre for European Social Research, we are working on a project where we classify a large database of data visualizations created by social scientists. Our aim is to identify key characteristics and features of these visualizations to learn about the state of the art in social science data visualization and the level of complexity currently given in this field.\nAt the moment, we are developing a Shiny App with R which will be used to manually annotate all visualizations in the database.\nMore updates will follow for this project as we continue."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "For my upcoming professional chapter, I am looking forward to explore opportunities in Data Analysis and Data Science. If you are interested in connecting, learning more about my work, or discussing potential collaborations, feel free to reach out!"
  },
  {
    "objectID": "index.html#you-can-contact-me-here",
    "href": "index.html#you-can-contact-me-here",
    "title": "About",
    "section": "You can contact me here:",
    "text": "You can contact me here:"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "In my previous work and projects, I‚Äôve applied statistical analysis methods, ranging from descriptive techniques to advanced methodologies. One of my focus in advanced methods is Natural Language Processing, where I use methods of machine learning and deep learning techniques to generate knowledge about textual data.\nIn terms of machine learning, I mostly enjoy and appreciate supervised machine learning (Andrew Ng‚Äôs explantion on why these architectures are the most useful), I have also worked in projects successfully utilizing unsupervised techniques (e.g., topic modeling) and large language models (e.g., zero-shot prompting with GPT-3.5)."
  },
  {
    "objectID": "Ohne Titel.html",
    "href": "Ohne Titel.html",
    "title": "Portfolio",
    "section": "",
    "text": "In my previous work and projects, I‚Äôve applied statistical analysis methods, ranging from descriptive techniques to advanced methodologies. One of my focus in advanced methods is Natural Language Processing, where I use methods of machine learning and deep learning techniques to generate knowledge about textual data.\nIn terms of machine learning, I mostly enjoy and appreciate supervised machine learning (Andrew Ng‚Äôs explantion on why these architectures are the most useful), I have also worked in projects successfully utilizing unsupervised techniques (e.g., topic modeling) and large language models (e.g., zero-shot prompting with GPT-3.5)."
  },
  {
    "objectID": "Ohne Titel.html#panel-1",
    "href": "Ohne Titel.html#panel-1",
    "title": "Four Panel Data Visualization",
    "section": "Panel 1",
    "text": "Panel 1\n\nHeading 1\nSome text about the data visualization.\n\n# Insert your data visualization code here for Panel 1\n# Example using ggplot2\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n# Insert your data visualization code here for Panel 2\n# Example using ggplot2\nlibrary(ggplot2)\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point()"
  },
  {
    "objectID": "portfolio.html#entropy-analysis",
    "href": "portfolio.html#entropy-analysis",
    "title": "Portfolio",
    "section": "Entropy analysis",
    "text": "Entropy analysis\n\nSummary\nQuestionnaires sometimes include open-ended questions (and if we had better access to methods for analyzing their open-ended responses, we would see them even more often). In this project, I investigated whether open-ended answers are useful in terms of their information content. This research required me to define and to operationalize information content (i.e., the amount of information given in an answer). Among various measures, I proposed the use of a measure from information theory, the entropy of a response. The below figure is my visualization for exemplary survey answers alongside their entropy score.\n\n\n\nFigure 1: Example Survey Answers by Entropy.Note. The topic ofthis questionnaire item concerned environmental issues."
  },
  {
    "objectID": "portfolio.html#topic-model-analysis",
    "href": "portfolio.html#topic-model-analysis",
    "title": "Portfolio",
    "section": "Topic Model Analysis",
    "text": "Topic Model Analysis\n\nSummary\nTopic Modeling is a widely famous and often used method from unspervised Machine Learning aimin at exploring given topics in a corpus. Throughout my work, I have used topic models at various instances, most often for the usecase of exploring and learning about new, unstructured datasets. I follow the devlopment of topic models since many years, andused various of them (e.g., LDA, BERTopic), but just recently I started working on identifying topics with the help of a Large Language Model. In particular, I use GPT-3.5-turbo and prompt it too identify, label and describe topics in a given corpus. I don‚Äôt provide any examples of topics, hence this is also called zero-shot prompting.\nThe below dataset shows the main topics alongside des\n\n\n\n\n\n\n\n\n\n\n\nTopic Name\n\n\nTopic description\n\n\n\n\n\n\nClimate Change Denial\n\n\nSome respondents express skepticism about human-caused climate change and the ability of humans to make a significant impact on the environment.\n\n\n\n\nGlobal Leadership\n\n\nConcerns about the lack of leadership and cooperation among world leaders in addressing climate change.\n\n\n\n\nPollution and Emissions\n\n\nReferences to major polluting countries like China and India, as well as the normalization of pollution.\n\n\n\n\nInequality and Suspicion\n\n\nMentions of inequality among nations and suspicion of other countries‚Äô actions.\n\n\n\n\nCapitalism and Profit Motive\n\n\nDoubts about countries prioritizing profit over environmental safety under capitalism.\n\n\n\n\nInternational Efforts\n\n\nComments on international agreements like the Paris Agreement and the need for global cooperation.\n\n\n\n\nLack of Information\n\n\nAcknowledgment of insufficient information to form a confident opinion on climate change actions."
  },
  {
    "objectID": "portfolio.html#automatic-speech-recognition",
    "href": "portfolio.html#automatic-speech-recognition",
    "title": "Portfolio",
    "section": "Automatic Speech Recognition",
    "text": "Automatic Speech Recognition\n\nSummary\nSpoken language provides analysts and researchers with very dense and rich amounts of information. For example speech, in contrast to written language, can deliver additional information through characteristics such as speed, intonation and volume, as well as other non-verbal elements, such as laughter, pauses and sighs. This led me to collect voice answers in various of my survey projects and for one of my white papers I compared different algorithms for automatic speech recognition. In this comparison I find that whisper, a speech-to-text algorithm provided by openAI performs best in terms of word error rates.\n\n\n\nFigure 2: Waveform of an exemplary audio file and the unit of analyis.\n\n\n\n\n\nFigure 3: Word Error Rates across different ASR algorithms."
  },
  {
    "objectID": "portfolio.html#fine-tuning-of-a-bert-model-to-assign-open-ended-survey-answers-to-pre-defined-categories",
    "href": "portfolio.html#fine-tuning-of-a-bert-model-to-assign-open-ended-survey-answers-to-pre-defined-categories",
    "title": "Portfolio",
    "section": "Fine-tuning of a BERT model to assign open-ended survey answers to pre-defined categories",
    "text": "Fine-tuning of a BERT model to assign open-ended survey answers to pre-defined categories\n\nSummary\nThe below shows results from one of my research projects in which I investigated various methods of supervised machine learning approaches to assign open-ended survey answers to pre-defined categories. For example, I was interested in detecting whether a certain survey answer is given in positive, negative or neutral sentiment.\n\n\n\nFigure 4: Sentiment classified with a fine-tuned BERT model for different survey items.\n\n\n\n\n\nFigure 5: Results from multivariate regression analysis."
  },
  {
    "objectID": "Ohne Titel.html#entropy-analysis",
    "href": "Ohne Titel.html#entropy-analysis",
    "title": "Portfolio",
    "section": "Entropy Analysis",
    "text": "Entropy Analysis\n\nSummary\nQuestionnaires sometimes include open-ended questions (and if we had better access to methods for analyzing their open-ended responses, we would see them even more often). In this project, I investigated whether open-ended answers are useful in terms of their information content. This research required me to define and to operationalize information content (i.e., the amount of information given in an answer). Among various measures, I proposed the use of a measure from information theory, the entropy of a response. The below figure is my visualization for exemplary survey answers alongside their entropy score.\n\n\n\nFigure 1: Example Survey Answers by Entropy.Note. The topic ofthis questionnaire item concerned environmental issues."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nüë©üèΩ‚Äçüíª For the last 4 years I have been working as a Quantitative Researcher at the Mannheim Centre for European Social Research (MZES) at the University of Mannheim. Previously I was a Visiting Researcher at the Berlin Social Science Center, a Research Assistant at the GESIS Leibniz Institute for the Social Sciences, and an Intern as well as Student Research Assistant at the Fraunhofer Institute for Industrial Engineering.\n\nFor a detailed overview of previous activities, please find my full CV here."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nüè´üéì I am trained in social sciences and in particular at the intersection of social science survey research, statistics and data science. To achieve this interdisciplinary combination of skills, I studied Social Sciences and Sociology at the University of Stuttgart and Mannheim. Additionally, in April 2024 I completed my doctorate in Quantitative Sociology at the Department for Social Data Science and Methodology at the University of Mannheim.\n\nThe slides of my disputation talk can be found here."
  },
  {
    "objectID": "work-samples.html",
    "href": "work-samples.html",
    "title": "Work samples",
    "section": "",
    "text": "In my previous work and projects, I have applied a wide spectrum of statistical analysis methods, ranging from descriptive techniques to advanced methodologies. Over the last three years, I have placed special emphasis on methods in the field of Natural Language Processing (NLP).\nI am confident in using these methods due to my detailed theoretical knowledge acquired through coursework in my studies (3 years Bachelor, 2 years Master), which was heavily focused on statistics. Additionally, I have applied these methods in my role as a quantitative researcher for the past 4 years. My practical experience is complemented by my teaching activities in the field of statistics for undergraduate and graduate students, specifically in multivariate statistics and NLP, which have further trained my skills in explaining complex issues in easier terms.\nBelow is a selection of methods I have used in the past:\nPlease continue reading below for examples of my past work where I applied these methods."
  },
  {
    "objectID": "work-samples.html#entropy-analysis",
    "href": "work-samples.html#entropy-analysis",
    "title": "Work samples",
    "section": "Entropy analysis",
    "text": "Entropy analysis\n\nSummary\nQuestionnaires sometimes include open-ended questions (and if we had better access to methods for analyzing their open-ended responses, we would see them even more often). In this project, I investigated whether open-ended answers are useful in terms of their information content. This research required me to define and to operationalize information content (i.e., the amount of information given in an answer). Among various measures, I proposed the use of a measure from information theory, the entropy of a response. The below figure is my visualization for exemplary survey answers alongside their entropy score.\n\n\n\nFigure 1: Example Survey Answers by Entropy.Note. The topic ofthis questionnaire item concerned environmental issues."
  },
  {
    "objectID": "work-samples.html#topic-model-analysis",
    "href": "work-samples.html#topic-model-analysis",
    "title": "Work samples",
    "section": "Topic Model Analysis",
    "text": "Topic Model Analysis\n\nSummary\nTopic Modeling is a widely famous and often used method from unspervised Machine Learning aimin at exploring given topics in a corpus. Throughout my work, I have used topic models at various instances, most often for the usecase of exploring and learning about new, unstructured datasets. I follow the devlopment of topic models since many years, andused various of them (e.g., LDA, BERTopic), but just recently I started working on identifying topics with the help of a Large Language Model. In particular, I use GPT-3.5-turbo and prompt it too identify, label and describe topics in a given corpus. I don‚Äôt provide any examples of topics, hence this is also called zero-shot prompting.\nThe below dataset shows the main topics alongside descriptions.\n\n\n\n\n\n\n\n\n\n\n\nTopic Name\n\n\nTopic description\n\n\n\n\n\n\nClimate Change Denial\n\n\nSome respondents express skepticism about human-caused climate change and the ability of humans to make a significant impact on the environment.\n\n\n\n\nGlobal Leadership\n\n\nConcerns about the lack of leadership and cooperation among world leaders in addressing climate change.\n\n\n\n\nPollution and Emissions\n\n\nReferences to major polluting countries like China and India, as well as the normalization of pollution.\n\n\n\n\nInequality and Suspicion\n\n\nMentions of inequality among nations and suspicion of other countries‚Äô actions.\n\n\n\n\nCapitalism and Profit Motive\n\n\nDoubts about countries prioritizing profit over environmental safety under capitalism.\n\n\n\n\nInternational Efforts\n\n\nComments on international agreements like the Paris Agreement and the need for global cooperation.\n\n\n\n\nLack of Information\n\n\nAcknowledgment of insufficient information to form a confident opinion on climate change actions."
  },
  {
    "objectID": "work-samples.html#automatic-speech-recognition",
    "href": "work-samples.html#automatic-speech-recognition",
    "title": "Work samples",
    "section": "Automatic Speech Recognition",
    "text": "Automatic Speech Recognition\n\nSummary\nSpoken language provides analysts and researchers with very dense and rich amounts of information. For example speech, in contrast to written language, can deliver additional information through characteristics such as speed, intonation and volume, as well as other non-verbal elements, such as laughter, pauses and sighs. This led me to collect voice answers in various of my survey projects and for one of my white papers I compared different algorithms for automatic speech recognition. In this comparison I find that whisper, a speech-to-text algorithm provided by openAI performs best in terms of word error rates.\n\n\n\nFigure 2: Waveform of an exemplary audio file and the unit of analyis.\n\n\n\n\n\nFigure 3: Word Error Rates across different ASR algorithms."
  },
  {
    "objectID": "work-samples.html#fine-tuning-of-a-bert-model-to-assign-open-ended-survey-answers-to-pre-defined-categories",
    "href": "work-samples.html#fine-tuning-of-a-bert-model-to-assign-open-ended-survey-answers-to-pre-defined-categories",
    "title": "Work samples",
    "section": "Fine-tuning of a BERT model to assign open-ended survey answers to pre-defined categories",
    "text": "Fine-tuning of a BERT model to assign open-ended survey answers to pre-defined categories\n\nSummary\nThe below shows results from one of my research projects in which I investigated various methods of supervised machine learning approaches to assign open-ended survey answers to pre-defined categories. For example, I was interested in detecting whether a certain survey answer is given in positive, negative or neutral sentiment.\n\n\n\nFigure 4: Sentiment classified with a fine-tuned BERT model for different survey items.\n\n\n\n\n\nFigure 5: Results from multivariate regression analysis."
  },
  {
    "objectID": "contact.html#you-can-contact-me-here",
    "href": "contact.html#you-can-contact-me-here",
    "title": "Contact",
    "section": "You can contact me here:",
    "text": "You can contact me here:\n\nEmail: landesvatterc@gmail.com,\nLinkedIn,\nGitHub."
  },
  {
    "objectID": "awards.html",
    "href": "awards.html",
    "title": "Awards",
    "section": "",
    "text": "I have twice received a Google Cloud Research Grant for delievering proof of concepts for social science projects.\n\n\nA Grant of 1,000 USD was awarded to me conduct research in the area of Computational Social Science. In particular, I was developing a novel approach to measure social trust by creating and testing more specific, situational self-report measures through a large-scale survey on social media. Utilizing Google Cloud Platform tools, I will process and analyze audio responses from thousands of participants using Automatic Speech Recognition and Natural Language APIs for text conversion and sentiment analysis. This project involves significant computational challenges, including handling and analyzing 24 GB of audio data and 266 hours of recordings. My work aims to improve trust measurement techniques and provides insights into trust dynamics beyond traditional survey methods. The final project made use of GCP tools, such as Speech-to-Text API, Compute engine, Natural Language API and BigQuery.\n\n\n\nTo follow up and deepen my previous work from the 2021 Research Grant, I was rewarded another 1,000 USD to continue working in this field. The focus of this work package was to leverage the Automatic Speech Recognition Capabilities of the respective GCP API. One of the main outcomes of this work was a unique and elaborated comparison of the performance of various Automatic Speech Recognition Algorithms."
  },
  {
    "objectID": "awards.html#google-cloud-research-grant",
    "href": "awards.html#google-cloud-research-grant",
    "title": "Awards",
    "section": "",
    "text": "I have twice received a Google Cloud Research Grant for delievering proof of concepts for social science projects.\n\n\nA Grant of 1,000 USD was awarded to me conduct research in the area of Computational Social Science. In particular, I was developing a novel approach to measure social trust by creating and testing more specific, situational self-report measures through a large-scale survey on social media. Utilizing Google Cloud Platform tools, I will process and analyze audio responses from thousands of participants using Automatic Speech Recognition and Natural Language APIs for text conversion and sentiment analysis. This project involves significant computational challenges, including handling and analyzing 24 GB of audio data and 266 hours of recordings. My work aims to improve trust measurement techniques and provides insights into trust dynamics beyond traditional survey methods. The final project made use of GCP tools, such as Speech-to-Text API, Compute engine, Natural Language API and BigQuery.\n\n\n\nTo follow up and deepen my previous work from the 2021 Research Grant, I was rewarded another 1,000 USD to continue working in this field. The focus of this work package was to leverage the Automatic Speech Recognition Capabilities of the respective GCP API. One of the main outcomes of this work was a unique and elaborated comparison of the performance of various Automatic Speech Recognition Algorithms."
  },
  {
    "objectID": "work-samples.html#footnotes",
    "href": "work-samples.html#footnotes",
    "title": "Work samples",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAndrew Ng‚Äôs explantion on why these architectures are the most useful‚Ü©Ô∏é"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "For a list of my publications, white papers and preprints please explore my Google Scholar Account."
  },
  {
    "objectID": "publications.html#spotlight-evaluation-of-asr-algorithms",
    "href": "publications.html#spotlight-evaluation-of-asr-algorithms",
    "title": "Publications",
    "section": "Spotlight: Evaluation of ASR algorithms",
    "text": "Spotlight: Evaluation of ASR algorithms\nOne of my most recent whitepapers describes a comparison of various automatic speech recognition methods. I conduct speech recognition with 4 of the most prominent algorithms (e.g., openAI whisper, NVIDIA NeMo, Google Cloud ASR) and compute word error rates to evaluate their performance.\nThe white paper can be found here. A full version is also under review at a high-quality journal at the moment.\n\nTechnology\nFor typesetting my texts and creating visualizations, I use the following technologies:\nText and Typesetting: R Markdown, R Pagedown, Quarto, Google Doc, MS Word.\nData Viz: R: ggplot2, plotly, shiny, Python: matplotlib, seaborn, Other: Miro, Canvas, Figma."
  },
  {
    "objectID": "publications.html#introduction-to-r-markdown-pagedown-quarto-and-reproducible-manuscripts",
    "href": "publications.html#introduction-to-r-markdown-pagedown-quarto-and-reproducible-manuscripts",
    "title": "Publications",
    "section": "Introduction to R Markdown, Pagedown, Quarto and Reproducible Manuscripts",
    "text": "Introduction to R Markdown, Pagedown, Quarto and Reproducible Manuscripts\nI enjoy working with words and language, and I am enthusiastic about outputting results in well-written formats, such as manuscripts. I believe that the presentation of information can significantly enhance its impact. This passion led me to collaborate with a colleague at the University of Mannheim on introducing typesetting techniques and document creation with R. Specifically, we have explored tools like Markdown, Pagedown, and Quarto to help students and other interested individuals discover various ways to format and present their results from statistical analyses, coursework, and other projects in engaging and professional formats.\nYou can find our introductions here: R Markdown, Pagedown, R Studio and Quarto.\nIn 2023 I was also invited by the Mannheim Open Science Office at the University of Mannheim to talk about the various features and benefits of reproducible manuscripts. You can explore the slides here."
  }
]